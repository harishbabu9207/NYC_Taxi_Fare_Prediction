---
title: "R Notebook"
output: html_notebook
---

```{r}
library(mlbench)
library(caret)
library(caretEnsemble)
```

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3,
                     verboseIter = T,
                     classProbs = T,
                     sampling = "smote",
                     summaryFunction = twoClassSummary,
                     savePredictions = T)
seed <- 7
metric <- "Accuracy"
# C5.0
set.seed(seed)
fit.c50 <- train(fraud~., data=data_train[ c(2:19,22) ], method="C5.0", metric=metric, trControl=control,na.action=na.omit)
# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm <- train(fraud~., data=data_train[ c(2:19,22) ], method="gbm", metric=metric, trControl=control, verbose=FALSE,na.action=na.omit)
# summarize results
boosting_results <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))
summary(boosting_results)
dotplot(boosting_results)
```

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3,
                     verboseIter = T,
                     classProbs = T,
                     sampling = "smote",
                     summaryFunction = twoClassSummary,
                     savePredictions = T)
levels(data_train$fraud) <- c("first_class", "second_class")
algorithmList <- c('rpart', 'glm', 'knn', 'svmRadial')
set.seed(seed)
naRows <- apply(data_train[ c(2:19,22) ], 1, function(x) any(is.na(x)))
dtrain= data_train[!naRows,]
models <- caretList(fraud~., data=dtrain[ c(2:19,22) ], trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
modelCor(results)
splom(results)

stackControl <- trainControl(method="repeatedcv", number=10, savePredictions=TRUE, classProbs=TRUE)
set.seed(seed)
stack.glm <- caretStack(models, method="glm",  trControl=stackControl)
print(stack.glm)
set.seed(seed)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
stack.rf
```

```{r}

library(readr)
library(dplyr)
library(randomForest)
library(ggplot2)
library(Hmisc)
library(party)
library(MLmetrics)
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(devtools)
library(mlr)
library(parallel)
library(parallelMap)

data_train=read.csv("uconn_comp_2018_train.csv",colClasses = c("numeric","numeric" ,"factor" , "factor" , "numeric" ,"numeric" , "factor" , "factor" , "factor" , "factor" , "factor" , "factor" , "numeric" ,"factor","factor" ,"numeric","numeric","factor","numeric","factor","numeric","factor") )
glimpse(data_train)
data_test=read.csv("uconn_comp_2018_test.csv",colClasses = c("numeric","numeric" ,"factor" , "factor" , "numeric" ,"numeric" , "factor" , "factor" , "factor" , "factor" , "factor" , "factor" , "numeric" ,"factor","factor" ,"numeric","numeric","factor","numeric","factor","numeric","factor"))

lf=ceiling(nrow(data_train)*0.70)
data_train$fraud=factor(data_train$fraud)
train <- data_train[1:lf, ]
test <- data_train[lf:nrow(data_train), ]
length(train)

naRows <- apply(train[ c(2:19,22) ], 1, function(x) any(is.na(x)))
dtrain= train[!naRows,]

naRows <- apply(test[ c(2:19,22) ], 1, function(x) any(is.na(x)))
dtest=test[!naRows,]

naRows <- apply(data_test[ c(2:19,22) ], 1, function(x) any(is.na(x)))
ftest=data_test[!naRows,]
data_test[naRows,]


set.seed(1001)
vars=c("gender","marital_status","high_education_ind","address_change_ind","living_status","accident_site","past_num_of_claims","witness_present_ind","channel","policy_report_filed_ind","age_of_vehicle","vehicle_category","vehicle_color")
Dummydata=normalizeFeatures(dtrain[,-which(names(dtrain) %in% c("claim_number","channel","vehicle_price","vehicle_weight","age_of_driver","vehicle_category" , "vehicle_color","policy_report_filed_ind"))],method = "standardize")
TestDummydata=normalizeFeatures(dtest[,-which(names(dtest) %in% c("claim_number","channel","vehicle_price","vehicle_weight","age_of_driver","vehicle_category" , "vehicle_color","policy_report_filed_ind"))],method = "standardize")
fTestDummydata=normalizeFeatures(dtest[,-which(names(dtest) %in% c("claim_number","channel","vehicle_price","vehicle_weight","age_of_driver","vehicle_category" , "vehicle_color","policy_report_filed_ind"))],method = "standardize")
Traindummies <- vtreat::designTreatmentsZ(Dummydata, vars, 
                                   minFraction= 0,
                                   verbose=FALSE)
Testdummies <- vtreat::designTreatmentsZ(TestDummydata, vars, 
                                   minFraction= 0,
                                   verbose=FALSE)
fTestdummies <- vtreat::designTreatmentsZ(fTestDummydata, vars, 
                                   minFraction= 0,
                                   verbose=FALSE)

Traindummy=as.data.frame(cbind(vtreat::prepare(Traindummies,dtrain),dtrain$fraud,dtrain$safty_rating,dtrain$annual_income,dtrain$liab_prct,dtrain$claim_est_payout))
length(Traindummy)
names(Traindummy)[30]="claim_est_payout"
names(Traindummy)[29]="liab_prct"
names(Traindummy)[28]="annual_income"
names(Traindummy)[27]="safty_rating"
names(Traindummy)[26]="fraud"

Testdummy=as.data.frame(cbind(vtreat::prepare(Testdummies,dtest),dtest$fraud,dtest$safty_rating,dtest$annual_income,dtest$liab_prct,dtest$claim_est_payout))

names(Testdummy)[30]="claim_est_payout"
names(Testdummy)[29]="liab_prct"
names(Testdummy)[28]="annual_income"
names(Testdummy)[27]="safty_rating"
names(Testdummy)[26]="fraud"
View(Testdummy)

fTestdummy=as.data.frame(cbind(vtreat::prepare(fTestdummies,ftest),ftest$fraud,ftest$safty_rating,ftest$annual_income,ftest$liab_prct,ftest$claim_est_payout))

names(fTestdummy)[30]="claim_est_payout"
names(fTestdummy)[29]="liab_prct"
names(fTestdummy)[28]="annual_income"
names(fTestdummy)[27]="safty_rating"
names(fTestdummy)[26]="fraud"
```

```{r}

##xgboost
library(DMwR)

best_param = list()
best_seednumber = 1234
best_logloss = Inf
best_logloss_index = 0
##train_smote <- SMOTE(fraud ~ ., as.data.frame(Traindummy), perc.over = 20000, perc.under=100)

dtrain_X <- xgb.DMatrix(data = data.matrix(Traindummy[ c(3:25,27:30)  ]), label = as.numeric(Traindummy$fraud)-1)
dtest_X <- xgb.DMatrix(data = data.matrix(Testdummy[ c(3:25,27:30) ]), label = as.numeric(Testdummy$fraud)-1)
ftest_X <- xgb.DMatrix(data = data.matrix(fTestdummy[ c(3:25,27:30) ]), label = as.numeric(fTestdummy$fraud)-1)
iter=1

for (iter in 200) {
param <- list(objective = "binary:logistic",
          eval_metric = "error",
          max_depth = round(runif(1,1,20)),
          eta =runif(1,0,1),
          gamma = runif(1,0,20),
          lambda=runif(1,0.3,0.6),
          subsample = runif(1,0,1),
          colsample_bytree = runif(1,0,1), 
          min_child_weight = runif(1,1,10),
          max_delta_step = runif(1,1,10))

 seed.number = sample.int(10000, 1)[[1]]
    set.seed(seed.number)
   mdcv <- xgb.cv(data=dtrain_X, params = param, nthread=10, 
                    nfold=10, nrounds=1000,early_stopping_rounds = 10,
                    verbose = T)
  
   min_logloss = min(mdcv[["evaluation_log"]][["test_error_mean"]])
    min_logloss_index = which.min(mdcv[["evaluation_log"]][["test_error_mean"]])

  if (min_logloss < best_logloss) {
       best_logloss = min_logloss
      best_logloss_index = min_logloss_index
       best_seednumber = seed.number
       best_param = param
          }
   iter=iter+1
}
mdcv
set.seed(seed.number)
xgb <- xgboost(data = dtrain_X,params=best_param,nrounds=best_logloss_index)
md <- xgb.train(data=dtrain_X, params=param,nrounds=best_logloss_index)
md
preds_xgb <- predict(md, dtest_X,type="prob")
table(Testdummy$fraud,as.numeric(preds_xgb>0.5))
table(as.numeric(preds_xgb>0.5))
library("pROC")
auc(Testdummy$fraud,as.numeric(preds_xgb>0.5))


searchGridSubCol <- expand.grid(subsample = runif(1,0.3,1), 
                                colsample_bytree = c(0.6, 0.8, 1))
ntrees <- 100

#Build a xgb.DMatrix object
DMMatrixTrain <- xgb.DMatrix(data = yourMatrix, label = yourTarget)

rmseErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){

    #Extract Parameters to test
    currentSubsampleRate <- parameterList[["subsample"]]
    currentColsampleRate <- parameterList[["colsample_bytree"]]

    xgboostModelCV <- xgb.cv(data =  DMMatrixTrain, nrounds = ntrees, nfold = 5, showsd = TRUE, 
                           metrics = "rmse", verbose = TRUE, "eval_metric" = "rmse",
                           "objective" = "reg:linear", "max.depth" = 15, "eta" = 2/ntrees,                               
                           "subsample" = currentSubsampleRate, "colsample_bytree" = currentColsampleRate)

    xvalidationScores <- as.data.frame(xgboostModelCV)
    #Save rmse of the last iteration
    rmse <- tail(xvalidationScores$test.rmse.mean, 1)

    return(c(rmse, currentSubsampleRate, currentColsampleRate))

})


```



```

